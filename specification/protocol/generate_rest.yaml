openapi: 3.0.3
info:
  title: Open Inference API for text generation
  description: Open Inference API for text generation
  version: 1.0.0
components:
  schemas:
    GenerateRequest:
      type: object
      properties:
        prompt:
          type: string
        parameters:
          $ref: '#/components/schemas/GenerateParameters'
    GenerateParameters:
      type: object
      properties:
        temperature:
          type: number
          "format": "float"
          "default": "null"
          "example": 0.5
          "nullable": true
          minimum: 0
          description: |-
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
        top_p:
          type: number
          "format": "float"
          nullable: true
          maximum: 1.0
          minimum: 0
          description: |-
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
        max_token:
          type: integer
          "format": "int32"
          default: 20
          minimum: 0
          maximum: 512
          description: The maximum number of tokens to generate in the completion.
        best_of:
          type: integer
          "format": "int32"
          minimum: 0
          example: 1
          description: |-
            Generates best_of completions server-side and returns the "best" (the one with the highest log probability per token).
        stop:
          type: array
          items:
            type: string
          maxItems: 4
          description: |-
            Up to 4 sequences where the API will stop generating further tokens.
        frequency_penalty:
          type: number
          format:  "float"
          example: 1.03
          minimum: 0
          description: |-
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    GenerateResponse:
      type: object
      required:
        - text
      properties:
        text:
          type: string
        tokens:
          type: array
          items:
            $ref: '#/components/schemas/Token'
        finishReason:
          $ref: '#/components/schemas/Token'
    GenerateStreamResponse:
      type: object
      required:
        - token
      properties:
        text:
          type: string
        token:
          $ref: '#/components/schemas/Token'
        finishReason:
          $ref: '#/components/schemas/Token'
    Token:
      type: object
      properties:
        id:
          type: integer
        text:
          type: string
        logprob:
          type: number
          format: "float"
    "FinishReason":
      "type": "string"
      "example": "length"
      "enum":
      - "length"
      - "stop"
paths:
  /v1/models/{model_name}/generate:
    post:
      parameters:
        - name: model_name
          in: path
          schema:
            type: string
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateRequest'
      responses:
        '200':
          description: generated text
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/GenerateResponse'
  /v1/models/{model_name}/generate_stream:
    post:
      parameters:
        - name: model_name
          in: path
          schema:
            type: string
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/GenerateRequest'
      responses:
        '200':
          description: generated text stream
          content:
            text/event-stream:
              schema:
                $ref: '#/components/schemas/GenerateStreamResponse'